{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## [비교] 사이킷런 모델과 케라스 모델\n",
    "\n",
    "#### [ 사이킷런]\n",
    "    1- 모델    sc = SGDClassifier( loss='log', max_iter=5 )\n",
    "                                log : 손실함수, 반복횟수 5번\n",
    "    2- 훈련    sc.fit( train_scaled, train_target )\n",
    "    3- 평가    sc.score( test_scaled, test_target ) \n",
    "    \n",
    "#### [ 케라스 ]  \n",
    "               dense = keras.layers.Dense( 10, activation='softmax', input_shape=(784,))\n",
    "    1- 모델    model = keras.Sequential(dense)\n",
    "               model.compile( loss='sparse_categorial_crossentropy', metrics='accuracy')\n",
    "                                 sparse_categorial_crossentropy : 손실함수\n",
    "    2- 훈련    model.fit( train_scaled, train_target, epochs=5 )\n",
    "                                                      반복횟수 : 5\n",
    "    3- 평가    model.evaluate( test_scaled, test_target )\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##  [  모델 구동  절차 ]\n",
    "\n",
    "0. 데이타 전처리\n",
    "\n",
    "1. 모델 생성\n",
    "   - model = Sequential() \n",
    "   - 딥러닝의 구조를 짜고 층을 설정하는 부분입니다. \n",
    "\n",
    "2. 모델 컴파일\n",
    "     - model.compile() \n",
    "     - 위에서 정해진 모델을 컴퓨터가 알아들을 수 있게끔 컴파일 하는 부분입니다. \n",
    "\n",
    "3. 모델 훈련\n",
    "    - model.fit()\n",
    "    - 모델을 실제로 수행하는 부분입니다. \n",
    "    \n",
    "4. 모델 검증\n",
    "\n",
    "5. 모델 예측\n",
    "\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) 데이타처리\n",
    "\n",
    "+ 주어진 폐암 수술 환자의 생존 여부 데이터는 총 470명의 환자에게서 17개의 정보를 정리한 것입니다.\n",
    "+ 각 정보를 ‘속성’이라고 부릅니다. 그리고 생존 여부를 클래스, 가로 한 줄에 해당하는 각 환자의 정보를 각각 ‘샘플’이라고 합니다. \n",
    "+ 주어진 데이터에는 총 470개의 샘플이 각각 17개씩의 속성을 가지고 있는 것입니다.\n",
    "\n",
    "<img src='./imgs/model3.PNG' width='500'/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)\n",
    "  \n",
    "# 준비된 수술 환자 데이터를 불러옴\n",
    "data_set = np.loadtxt(\"./dataset/ThoraricSurgery.csv\", delimiter=\",\")\n",
    "  \n",
    "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장\n",
    "X = data_set[:,0:17]\n",
    "Y = data_set[:,17]\n",
    "\n",
    "print(X[0])   # 17 개 항목\n",
    "print(Y[0])   # 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# [1]  모델 생성 - 리스트형\n",
    "\n",
    "\n",
    "\n",
    "# [2]  모델 생성 - add 함수\n",
    "\n",
    "\n",
    "\n",
    "# [참고] 모델 요약\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./imgs/model1.PNG' width='400'/>\n",
    "\n",
    "- 이제 Dense(30, input_dim=17, activation='relu') 부분을 더 살펴보겠습니다. \n",
    "- 30이라고 되어 있는 것은 이 층에 30개의 노드를 만들겠다는 것입니다. \n",
    "- input_dim이라는 변수가 나옵니다. 이는 입력 데이터에서 몇 개의 값을 가져올지를 정하는 것입니다. \n",
    "- keras는 입력층을 따로 만드는 것이 아니라, 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸합니다. \n",
    "- 우리가 다루고 있는 폐암 수술 환자의 생존 여부 데이터에는 17개의 입력 값들이 있습니다. 따라서 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다는 뜻입니다.\n",
    "\n",
    "\n",
    "\n",
    "    + 17개 입력데이타가 30개의 노드로 들어가니 17 * 30 이고 30 개에서 다시 하나의 출력층으로 가니 30을 더해\n",
    "        17 * 30 + 30 -> 540\n",
    "    + 30개의 노드에서 하나의 출력층으로 들어오고 하나의 출력층에서 나오니 30 + 1 -> 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (2) 모델 컴파일\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### - model.compile 부분은 앞서 지정한 모델이 효과적으로 구현될 수 있게 여러 가지 환경을 설정해 주면서 컴파일하는 부분입니다. \n",
    "##### - loss : 한 번 신경망이 실행될 때마다 오차 값을 추적하는 함수\n",
    "##### - optimizer : 오차를 어떻게 줄여 나갈지 정하는 함수\n",
    "\n",
    "\n",
    "   - 옵티마이저 : https://www.tensorflow.org/api_docs/python/tf/keras/optimizers\n",
    "   - 손실함수 :  https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "   - 평가함수 :  https://www.tensorflow.org/api_docs/python/tf/keras/metrics\n",
    "\n",
    "\n",
    "\n",
    "- 먼저 어떤 오차 함수를 사용할지를 정해야 합니다. 여기서는 평균 제곱 오차 함수(mean_squared_error)를 사용했습니다.\n",
    "- 그런데 경우에 따라서는 오차 함수를 바꾸면 더 좋은 효과를 나타내기도 합니다. 오차 함수에는 평균 제곱 오차 계열의 함수 외에도 교차 엔트로피 계열의 함수가 있습니다\n",
    "- 교차 엔트로피는 주로 분류 문제에서 많이 사용되는데, 특별히 예측 값이 참과 거짓 둘 중 하나인 형식일 때는 binary_crossentropy(이항 교차 엔트로피)를 씁니다. 지금 구하고자 하는 것은 생존(1) 또는 사망(0) 둘 중 하나이므로 binary_crossentropy를 사용할 수 있는 좋은 예라고 할 수 있습니다.\n",
    "\n",
    "#### 교차엔트로피\n",
    "<img src='./imgs/model2.PNG' width='500'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## (3) 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 주어진 폐암 수술 환자의 생존 여부 데이터는 총 470명의 환자에게서 17개의 정보를 정리한 것입니다.\n",
    "+ 각 정보를 ‘속성’이라고 부릅니다. 그리고 생존 여부를 클래스, 가로 한 줄에 해당하는 각 환자의 정보를 각각 ‘샘플’이라고 합니다. \n",
    "+ 주어진 데이터에는 총 470개의 샘플이 각각 17개씩의 속성을 가지고 있는 것입니다.\n",
    "\n",
    "<img src='./imgs/model3.PNG' width='500'/>\n",
    "\n",
    "\n",
    "+ 학습 프로세스가 모든 샘플에 대해 한 번 실행되는 것을 1 epoch(‘에포크’라고 읽습니다)라고 합니다. \n",
    "+ 코드에서 epochs=100으로 지정한 것은 각 샘플이 처음부터 끝까지 100번 재사용될 때까지 실행을 반복하라는 뜻입니다.\n",
    "\n",
    "+ batch_size는 샘플을 한 번에 몇 개씩 처리할지를 정하는 부분으로 batch_size=10은 전체 470개의 샘플을 10개씩 끊어서 집어넣으라는 뜻이 됩니다. \n",
    "+ batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결괏값이 불안정해질 수 있습니다. 따라서 자신의 컴퓨터 메모리가 감당할 만큼의 batch_size를 찾아 설정해 주는 것이 좋습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (4) 결과\n",
    "\n",
    "+ 정확도(accuracy) 83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 출력\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## (5) 예측\n",
    "\n",
    "+ 마지막 데이타로 확인\n",
    "\n",
    "447\t8\t5.2\t4.1\t0\t0\t0\t0\t0\t0\t12\t0\t0\t0\t0\t0\t49               0 (사망)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_x = [[447,8,5.2,4.1,0,0,0,0,0,0,12,0,0,0,0,0,49]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측결과\n",
    "\n",
    "0에 가까움"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
